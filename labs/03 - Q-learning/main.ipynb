{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The original code that this notebook was based on can be found at https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit2/unit2.ipynb\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "N_BANDITS = 3\n",
    "N_EPISODES = 5_000\n",
    "N_MAX_STEPS_PER_EPISODE = 100\n",
    "\n",
    "SIGNAL_WIN = 0\n",
    "SIGNAL_LOST = 1\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditEnvironment:\n",
    "    def __init__(self, n_bandits):    \n",
    "        self.n_bandits = n_bandits\n",
    "        self.bandits = None\n",
    "        \n",
    "        # Each bandit hits the jackpot 0-20% of the time, randomly  \n",
    "        self.bandits = [random.randint(0, 20) / 100 for b in range(self.n_bandits)]\n",
    "        \n",
    "        # We modifiy this percentage to a higher value (e.g. 60%) for a randomly selected bandit\n",
    "        self.bandits[random.randint(0, n_bandits-1)] = 0.6\n",
    "        \n",
    "        print(\"Generated probabilities:\", [b for b in self.bandits])\n",
    "    \n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take the bandit that the agent has selected\n",
    "        p_win = self.bandits[action]\n",
    "        \n",
    "        # Pull the lever (generate a random result based on the probibilites of the selected bandit)\n",
    "        pull_result = np.random.choice([SIGNAL_WIN, SIGNAL_LOST], 1, p=[p_win, 1 - p_win])[0]\n",
    "\n",
    "        # Set the reward variable to 1 if the player hit the jackpot, 0 otherwise\n",
    "        if pull_result == SIGNAL_WIN:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        new_state = 0\n",
    "        done = False\n",
    "        \n",
    "        return new_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # TODO: Generate an n_states*n_actions matrix\n",
    "        self.q_table = None\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # TODO: Generate a random number on the [0, 1) interval\n",
    "        random_int = None\n",
    "        \n",
    "        # TODO: We exploit with (1-epsilon) probability\n",
    "        if random_int > epsilon:\n",
    "            action = None\n",
    "        # TODO: We explore with epsilon probability\n",
    "        else:\n",
    "            action = None\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, new_state, gamma):\n",
    "        # TODO\n",
    "        old_value = None\n",
    "        new_estimate = None\n",
    "        \n",
    "        # Update the approximation of the Q-value\n",
    "        self.q_table[state][action] = old_value + self.learning_rate * (new_estimate- old_value)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BanditEnvironment(n_bandits=N_BANDITS)\n",
    "\n",
    "agent = QLearningAgent(n_states=1, n_actions=N_BANDITS, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay_rate = 0.001\n",
    "gamma = 0.95\n",
    "\n",
    "rewards = []\n",
    "epsilon_history = []\n",
    "\n",
    "for episode in tqdm(range(N_EPISODES)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # We slowly decrease the value of epsilon while not allowing it to go below the value specified in min_epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(N_MAX_STEPS_PER_EPISODE):\n",
    "        action = agent.act(state=state, epsilon=epsilon)\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "        \n",
    "        agent.learn(state, action, reward, new_state, gamma)\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        # If done, finish the episode\n",
    "        if done:\n",
    "            break\n",
    "      \n",
    "        # Our state is the new state\n",
    "        state = new_state\n",
    "        \n",
    "    #print(f\"Total reward for episode {episode}: {total_reward}\")\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    epsilon_history.append(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(N_EPISODES), epsilon_history)\n",
    "plt.title(\"The value of epsilon in each episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(N_EPISODES), rewards)\n",
    "plt.title(\"The obtained reward in each episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothen(data):\n",
    "    return np.cumsum(data) / np.arange(len(rewards) + 1)[1:]\n",
    "\n",
    "rewards_smooth_window_5 = smoothen(rewards)\n",
    "\n",
    "plt.plot(range(N_EPISODES), rewards)\n",
    "plt.plot(range(N_EPISODES), rewards_smooth_window_5, color=\"orange\")\n",
    "plt.title(\"The obtained reward in each episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend([\"rewards\", \"averaged rewards\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a87336bbb3c9a66b469a40ccde4c42aa781e98e8d6be98142d8f7f9e469d8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
